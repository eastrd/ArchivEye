7. THE ENTROPY OF AN INFORMATION SOURCE

Consider a discrete source of the finite state type considered above. For each possible state i there will be a
set of probabilities p;(j) of producing the various possible symbols j. Thus there is an entropy H; for each
state. The entropy of the source will be defined as the average of these H; weighted in accordance with the
probability of occurrence of the states in question:

H=) PH;
i
= —)Pipi(j) log pil).
ij

This is the entropy of the source per symbol of text. If the Markoff process is proceeding at a definite time
rate there is also an entropy per second

H!=Y fii
i
where fj is the average frequency (occurrences per second) of state i. Clearly
H'=mH

where m is the average number of symbols produced per second. H or H' measures the amount of informa-
tion generated by the source per symbol or per second. If the logarithmic base is 2, they will represent bits
per symbol or per second.

If successive symbols are independent then H is simply — ¥ p;log p; where p; is the probability of sym-
bol i. Suppose in this case we consider a long message of N symbols. It will contain with high probability
about p,N occurrences of the first symbol, p2N occurrences of the second, etc. Hence the probability of this
particular message will be roughly

— »PiN poN PnN
P=P, Py ***Pn"

or

logp =NY pjilogp;
i

log p = —NH
it
yz eile
N

H is thus approximately the logarithm of the reciprocal probability of a typical long sequence divided by the
number of symbols in the sequence. The same result holds for any source. Stated more precisely we have
(see Appendix 3):

Theorem 3: Given any € > 0 and6 > 0, we can find an No such that the sequences of any length N > No
fall into two classes:

1. A set whose total probability is less than e.

2. The remainder, all of whose members have probabilities satisfying the inequality

logp7!
—_ -H| <6.
N

, logp_!
In other words we are almost certain to have

very close to H when N is large.

A closely related result deals with the number of sequences of various probabilities. Consider again the
sequences of length N and let them be arranged in order of decreasing probability. We define n(q) to be
the number we must take from this set starting with the most probable one in order to accumulate a total
probability g for those taken.

13
