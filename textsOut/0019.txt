This double process then encodes the original message into the same symbols but with an average compres-
sion ratio Z.

As a second example consider a source which produces a sequence of A’s and B’s with probability p for
A and gq for B. If p < q we have

H = —logp(1—p)'?
= —plogp(1—p)"'~?)/?
. e
= plog—.
P

In such a case one can construct a fairly good coding of the message on a 0, | channel by sending a special
sequence, say 0000, for the infrequent symbol A and then a sequence indicating the number of B’s following
it. This could be indicated by the binary representation with all numbers containing the special sequence
deleted. All numbers up to 16 are represented as usual; 16 is represented by the next binary number after 16
which does not contain four zeros, namely 17 = 10001, etc.

It can be shown that as p + 0 the coding approaches ideal provided the length of the special sequence is
properly adjusted.

PART I: THE DISCRETE CHANNEL WITH NOISE

11. REPRESENTATION OF A NOISY DISCRETE CHANNEL

We now consider the case where the signal is perturbed by noise during transmission or at one or the other
of the terminals. This means that the received signal is not necessarily the same as that sent out by the
transmitter. Two cases may be distinguished. If a particular transmitted signal always produces the same
received signal, 1.e., the received signal is a definite function of the transmitted signal, then the effect may be
called distortion. If this function has an inverse — no two transmitted signals producing the same received
signal — distortion may be corrected, at least in principle, by merely performing the inverse functional
operation on the received signal.

The case of interest here is that in which the signal does not always undergo the same change in trans-
mission. In this case we may assume the received signal E to be a function of the transmitted signal S and a
second variable, the noise NV.

E=f (S N )

The noise is considered to be a chance variable just as the message was above. In general it may be repre-
sented by a suitable stochastic process. The most general type of noisy discrete channel we shall consider
is a generalization of the finite state noise-free channel described previously. We assume a finite number of
states and a set of probabilities
Pa,i (8 ? J ) :

This is the probability, if the channel is in state a and symbol i is transmitted, that symbol j will be received
and the channel left in state G. Thus a and ( range over the possible states, i over the possible transmitted
signals and j over the possible received signals. In the case where successive symbols are independently per-
turbed by the noise there is only one state, and the channel is described by the set of transition probabilities
pi(j), the probability of transmitted symbol i being received as j.

If a noisy channel is fed by a source there are two statistical processes at work: the source and the noise.
Thus there are a number of entropies that can be calculated. First there is the entropy H(x) of the source
or of the input to the channel (these will be equal if the transmitter is non-singular). The entropy of the
output of the channel, i.e., the received signal, will be denoted by H(y). In the noiseless case H(y) = H(x).
The joint entropy of input and output will be H(xy). Finally there are two conditional entropies H,(y) and
H,(x), the entropy of the output when the input is known and conversely. Among these quantities we have
the relations

H(x,y) = H(x) + Hy(y) = H(y) + Ay).

All of these entropies can be measured on a per-second or a per-symbol basis.

19
