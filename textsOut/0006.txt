pi(j) and the digram probabilities p(i, j) are related by the following formulas:
Pi) = Verli./) = LPG.) = Lei
j j j
P(i J) = P@pi(s)

Yr) = Le) =D.) = 1.

As a specific example suppose there are three letters A, B, C with the probability tables:

Pid) j i | pli) Pi) j
A BC A B C
4 1 9 4 1
ali to Bla als Bob
a ee
Cl> 3 w Cl a Cla tm Ts

A typical message from this source is the following:
ABBABABABABABABBBABBBBBABABABABABBBACACAB
BABBBBABBABACBBBABA.

The next increase in complexity would involve trigram frequencies but no more. The choice of
a letter would depend on the preceding two letters but not on the message before that point. A
set of trigram frequencies p(i, j,k) or equivalently a set of transition probabilities p;j(k) would
be required. Continuing in this way one obtains successively more complicated stochastic pro-
cesses. In the general n-gram case a set of n-gram probabilities p(i1,i2,...,i,) or of transition
probabilities pj, i,.,...,,,_,(in) is required to specify the statistical structure.

(D) Stochastic processes can also be defined which produce a text consisting of a sequence of
“words.” Suppose there are five letters A, B, C, D, E and 16 “words” in the language with
associated probabilities:

0A .16BEBE .11CABED .04DEB
.04 ADEB- .04 BED .05 CEED .15 DEED
05 ADEE .02BEED  .08 DAB .01 EAB

.01BADD .05CA .04 DAD .05 EE

Suppose successive “words” are chosen independently and are separated by a space. A typical
message might be:

DAB EE A BEBE DEED DEB ADEE ADEE EE DEB BEBE BEBE BEBE ADEE BED DEED
DEED CEED ADEE A DEED DEED BEBE CABED BEBE BED DAB DEED ADEB.

If all the words are of finite length this process is equivalent to one of the preceding type, but
the description may be simpler in terms of the word structure and probabilities. We may also
generalize here and introduce transition probabilities between words, etc.

These artificial languages are useful in constructing simple problems and examples to illustrate vari-
ous possibilities. We can also approximate to a natural language by means of a series of simple artificial
languages. The zero-order approximation is obtained by choosing all letters with the same probability and
independently. The first-order approximation is obtained by choosing successive letters independently but
each letter having the same probability that it has in the natural language.> Thus, in the first-order ap-
proximation to English, E is chosen with probability .12 (its frequency in normal English) and W with
probability .02, but there is no influence between adjacent letters and no tendency to form the preferred

>Letter, digram and trigram frequencies are given in Secret and Urgent by Fletcher Pratt, Blue Ribbon Books, 1939. Word frequen-
cies are tabulated in Relative Frequency of English Speech Sounds, G. Dewey, Harvard University Press, 1923.
