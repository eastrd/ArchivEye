. C
As N increases 6, \ and wy approach zero and the rate approaches —.

Another method of performing this coding and thereby proving the theorem can be described as follows:
Arrange the messages of length N in order of decreasing probability and suppose their probabilities are
Pi > P2 > P3--: > Pn. Let Ps = yy! pi; that is P, is the cumulative probability up to, but not including, ps.
We first encode into a binary system. The binary code for message s is obtained by expanding P, as a binary
number. The expansion is carried out to ms places, where m, is the integer satisfying:

1 1
logy — <ms <1+log, —.
Ps s
Thus the messages of high probability are represented by short codes and those of low probability by long
codes. From these inequalities we have

1 1
aim SPs < omaT
The code for P; will differ from all succeeding ones in one or more of its ms places, since all the remaining
P; are at least * larger and their binary expansions therefore differ in the first ms; places. Consequently all
the codes are different and it is possible to recover the message from its code. If the channel sequences are
not already sequences of binary digits, they can be ascribed binary numbers in an arbitrary fashion and the
binary code thus translated into signals suitable for the channel.
The average number H’ of binary digits used per symbol of original message is easily estimated. We
have

H'= ~ Lmps
But,
+ ¥ (tos: =), < 1S naps < <y (1 + log, ~)p,
N Ds? N “ON Ds?
and therefore, 1
Ul
Gy <H <Gy+ N

As N increases Gy approaches H, the entropy of the source and H’ approaches H.

We see from this that the inefficiency in coding, when only a finite delay of N symbols is used, need
not be greater than x plus the difference between the true entropy H and the entropy Gy calculated for
sequences of length NV. The per cent excess time needed over the ideal is therefore less than

This method of encoding is substantially the same as one found independently by R. M. Fano.’ His
method is to arrange the messages of length N in order of decreasing probability. Divide this series into two
groups of as nearly equal probability as possible. If the message is in the first group its first binary digit
will be 0, otherwise 1. The groups are similarly divided into subsets of nearly equal probability and the
particular subset determines the second binary digit. This process is continued until each subset contains
only one message. It is easily seen that apart from minor differences (generally in the last digit) this amounts
to the same thing as the arithmetic process described above.

10. DISCUSSION AND EXAMPLES

In order to obtain the maximum power transfer from a generator to a load, a transformer must in general be
introduced so that the generator as seen from the load has the load resistance. The situation here is roughly
analogous. The transducer which does the encoding should match the source to the channel in a statistical
sense. The source as seen from the channel through the transducer should have the same statistical structure

Technical Report No. 65, The Research Laboratory of Electronics, M.L.T., March 17, 1949.

17
