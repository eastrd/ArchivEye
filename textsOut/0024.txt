possible ways (using, however, only the high probability group of inputs as determined by the source S9)
and average the frequency of errors for this large class of possible coding systems. This is the same as
calculating the frequency of errors for a random association of the messages and channel inputs of duration
T. Suppose a particular output y; is observed. What is the probability of more than one message in the set
of possible causes of y;? There are 27* messages distributed at random in 27”) points. The probability of

a particular point being a message is thus
QT (R-H(x))

The probability that none of the points in the fan is a message (apart from the actual originating message) is

oT Hy(x)

P= [1—27-H)]

Now R < A(x) — Hy(x) so R— H(x) = —H,(x) — 7 with 7 positive. Consequently

Hy(x)

P= [1—2-TH Tay"

approaches (as T — 0)
1-2-7,

Hence the probability of an error approaches zero and the first part of the theorem is proved.

The second part of the theorem is easily shown by noting that we could merely send C bits per second
from the source, completely neglecting the remainder of the information generated. At the receiver the
neglected part gives an equivocation H(x) — C and the part transmitted need only add e. This limit can also
be attained in many other ways, as will be shown when we consider the continuous case.

The last statement of the theorem is a simple consequence of our definition of C. Suppose we can encode
a source with H(x) =C-+a in such a way as to obtain an equivocation H,(x) = a— with € positive. Then
R=H(x) =C+aand

H(x) —H,(x) =C+e

with € positive. This contradicts the definition of C as the maximum of H(x) — H,(x).

Actually more has been proved than was stated in the theorem. If the average of a set of numbers is
within € of of their maximum, a fraction of at most ./e can be more than ,/e below the maximum. Since € is
arbitrarily small we can say that almost all the systems are arbitrarily close to the ideal.

14. DISCUSSION

The demonstration of Theorem 11, while not a pure existence proof, has some of the deficiencies of such
proofs. An attempt to obtain a good approximation to ideal coding by following the method of the proof is
generally impractical. In fact, apart from some rather trivial cases and certain limiting situations, no explicit
description of a series of approximation to the ideal has been found. Probably this is no accident but is
related to the difficulty of giving an explicit construction for a good approximation to a random sequence.

An approximation to the ideal would have the property that if the signal is altered in a reasonable way
by the noise, the original can still be recovered. In other words the alteration will not in general bring it
closer to another reasonable signal than the original. This is accomplished at the cost of a certain amount of
redundancy in the coding. The redundancy must be introduced in the proper way to combat the particular
noise structure involved. However, any redundancy in the source will usually help if it is utilized at the
receiving point. In particular, if the source already has a certain redundancy and no attempt is made to
eliminate it in matching to the channel, this redundancy will help combat noise. For example, in a noiseless
telegraph channel one could save about 50% in time by proper encoding of the messages. This is not done
and most of the redundancy of English remains in the channel symbols. This has the advantage, however,
of allowing considerable noise in the channel. A sizable fraction of the letters can be received incorrectly
and still reconstructed by the context. In fact this is probably not a bad approximation to the ideal in many
cases, since the statistical structure of English is rather involved and the reasonable English sequences are
not too far (in the sense required for the theorem) from a random selection.

24
