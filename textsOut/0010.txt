where p; is the probability of the component source Lj.

Physically the situation represented is this: There are several different sources L;, Lz, L3,... which are
each of homogeneous statistical structure (i.e., they are ergodic). We do not know a priori which is to be
used, but once the sequence starts in a given pure component Lj, it continues indefinitely according to the
statistical structure of that component.

As an example one may take two of the processes defined above and assume p; = .2 and po = .8. A
sequence from the mixed source

L=.2L,+ .8L2

would be obtained by choosing first L; or Lo with probabilities .2 and .8 and after this choice generating a
sequence from whichever was chosen.

Except when the contrary is stated we shall assume a source to be ergodic. This assumption enables one
to identify averages along a sequence with averages over the ensemble of possible sequences (the probability
of a discrepancy being zero). For example the relative frequency of the letter A in a particular infinite
sequence will be, with probability one, equal to its relative frequency in the ensemble of sequences.

If P; is the probability of state i and p;(/) the transition probability to state j, then for the process to be
stationary it is clear that the P; must satisfy equilibrium conditions:

Pj) = YPipild)-

In the ergodic case it can be shown that with any starting conditions the probabilities P;(’) of being in state
j after N symbols, approach the equilibrium values as N — 9.

6. CHOICE, UNCERTAINTY AND ENTROPY

We have represented a discrete information source as a Markoff process. Can we define a quantity which
will measure, in some sense, how much information is “produced” by such a process, or better, at what rate
information is produced?

Suppose we have a set of possible events whose probabilities of occurrence are p1,p2,---,;Pn. These
probabilities are known but that is all we know concerning which event will occur. Can we find a measure
of how much “choice” is involved in the selection of the event or of how uncertain we are of the outcome?

If there is such a measure, say H(p1, p2,.--, Pn), it is reasonable to require of it the following properties:

1. A should be continuous in the pj.

2. If all the p; are equal, p; = 1, then H should be a monotonic increasing function of n. With equally
likely events there is more choice, or uncertainty, when there are more possible events.

3. If a choice be broken down into two successive choices, the original H should be the weighted sum
of the individual values of H. The meaning of this is illustrated in Fig. 6. At the left we have three

1/2 1p 1/2
1/3
2/3
~: 1/3
1/301/6

Fig. 6—Decomposition of a choice from three possibilities.

possibilities py = 5; P2= i P= i. On the right we first choose between two possibilities each with
probability 3; and if the second occurs make another choice with probabilities 2, ;. The final results
have the same probabilities as before. We require, in this special case, that

laid 1 iy. 1/2 1
A(5,3,5) =H(5,3) + 3A (3, 9)-

The coefficient 5 is because this second choice only occurs half the time.

10
