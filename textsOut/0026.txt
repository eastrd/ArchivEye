1
Q=——.

__ Bo
P= B42 B+2

The channel capacity is then
B+2
a

Note how this checks the obvious values in the cases p = 1 and p = 5: In the first, G = 1 and C = log3,
which is correct since the channel is then noiseless with three possible symbols. If p = 3 8B =2 and
C = log2. Here the second and third symbols cannot be distinguished at all and act together like one
symbol. The first symbol is used with probability P = 5 and the second and third together with probability
- This may be distributed between them in any desired way and still achieve the maximum capacity.

For intermediate values of p the channel capacity will lie between log2 and log3. The distinction
between the second and third symbols conveys some information but not as much as in the noiseless case.
The first symbol is used somewhat more frequently than the other two because of its freedom from noise.

C = log

16. THE CHANNEL CAPACITY IN CERTAIN SPECIAL CASES

If the noise affects successive channel symbols independently it can be described by a set of transition
probabilities p;;. This is the probability, if symbol 7 is sent, that j will be received. The maximum channel
rate is then given by the maximum of

-YP Pipijlogy Pipi +) Pipi log pij
i,j i ij
where we vary the P; subject to ) P; = 1. This leads by the method of Lagrange to the equations,

Psj
Ds; log =p s=1,2,....
» TS YiPipij

Multiplying by P; and summing on s shows that 4 = C. Let the inverse of ps; (if it exists) be hy so that
Ys hs Psj = 51j- Then:
Y hs Psjlog ps; _ log) Pipi = CY hy.

:

Sy]

Hence:
Papin = exp [-CY hs + Asrpsj log ps i|
i 5 8]
or,

P= hin exp [-CY hs + Vhs Psi log ps,| .
s SJ

This is the system of equations for determining the maximizing values of P;, with C to be determined so
that ) P; = 1. When this is done C will be the channel capacity, and the P; the proper probabilities for the
channel symbols to achieve this capacity.

If each input symbol has the same set of probabilities on the lines emerging from it, and the same is true
of each output symbol, the capacity can be easily calculated. Examples are shown in Fig. 12. In such a case
H,(y) is independent of the distribution of probabilities on the input symbols, and is given by — ¥ p;log p;
where the p; are the values of the transition probabilities from any input symbol. The channel capacity is

Max[H(y) — H,(y)] = MaxH(y) + Yi log pi.

The maximum of H(y) is clearly logm where m is the number of output symbols, since it is possible to make
them all equally probable by making the input symbols equally probable. The channel capacity is therefore

C= logm+ ¥° pilog pi.

26
