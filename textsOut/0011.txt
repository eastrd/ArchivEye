In Appendix 2, the following result is established:

Theorem 2: The only H satisfying the three above assumptions is of the form:

H=-K

where K is a positive constant.

n
pilog pi
i=1

This theorem, and the assumptions required for its proof, are in no way necessary for the present theory.
It is given chiefly to lend a certain plausibility to some of our later definitions. The real justification of these
definitions, however, will reside in their implications.

Quantities of the form H =—Y p; log p; (the constant K merely amounts to a choice of a unit of measure)
play a central role in information theory as measures of information, choice and uncertainty. The form of H
will be recognized as that of entropy as defined in certain formulations of statistical mechanics® where p; is
the probability of a system being in cell i of its phase space. H is then, for example, the H in Boltzmann’s
famous H theorem. We shall call H = —¥ p;log p; the entropy of the set of probabilities p;,..., py. If xis a
chance variable we will write H(x) for its entropy; thus x is not an argument of a function but a label for a
number, to differentiate it from H(y) say, the entropy of the chance variable y.

The entropy in the case of two possibilities with probabilities p and g = 1 — p, namely

H = —(plogp + qlogq)

is plotted in Fig. 7 as a function of p.

1.0

~ LY \

Fig. 7—Entropy in the case of two possibilities with probabilities p and (1— p).

The quantity H has a number of interesting properties which further substantiate it as a reasonable

measure of choice or information.

1. H = Oif and only if all the p; but one are zero, this one having the value unity. Thus only when we

are certain of the outcome does H vanish. Otherwise H is positive.

2. For a given n, H is a maximum and equal to logn when all the p; are equal (..e., +). This is also

intuitively the most uncertain situation.

8See, for example, R. C. Tolman, Principles of Statistical Mechanics, Oxford, Clarendon, 1938.

11
