Theorem 4:

Lim logn(q)

N-co

=H

when q does not equal 0 or 1.

We may interpret logn(q) as the number of bits required to specify the sequence when we consider only

logn(q)

the most probable sequences with a total probability g. Then ° is the number of bits per symbol for

the specification. The theorem says that for large N this will be independent of g and equal to H. The rate
of growth of the logarithm of the number of reasonably probable sequences is given by H, regardless of our
interpretation of “reasonably probable.” Due to these results, which are proved in Appendix 3, it is possible
for most purposes to treat the long sequences as though there were just 2 of them, each with a probability
2-HN

The next two theorems show that H and H' can be determined by limiting operations directly from
the statistics of the message sequences, without reference to the states and transition probabilities between
states.

Theorem 5: Let p(B;) be the probability of a sequence B; of symbols from the source. Let
1
Gu=—y Y' p(Bi) log p(B)
i

Where the sum is over all sequences B; containing N symbols. Then Gy is a monotonic decreasing function
of N and

Lim Gn =H.

N-co

Theorem 6: Let p(Bi,S;) be the probability of sequence B; followed by symbol S; and pzg,(Sj) =
p(Bi,S;)/p(Bi) be the conditional probability of S; after B;. Let

Fy = — ye P(Bi.5;) log pp; (Sj)

ij

where the sum is over all blocks B; of N — 1 symbols and over all symbols S;. Then Fy is a monotonic
decreasing function of N,

Fy = NGy —(N-1)Gw-1,

iy
Gv=> Fh;
Nia
Fy < Gy,

and Limy 5. Fy = H.

These results are derived in Appendix 3. They show that a series of approximations to H can be obtained
by considering only the statistical structure of the sequences extending over 1,2,...,N symbols. Fy is the
better approximation. In fact Fy is the entropy of the N™ order approximation to the source of the type
discussed above. If there are no statistical influences extending over more than N symbols, that is if the
conditional probability of the next symbol knowing the preceding (N — 1) is not changed by a knowledge of
any before that, then Fy = H. Fy of course is the conditional entropy of the next symbol when the (N — 1)
preceding ones are known, while Gy is the entropy per symbol of blocks of N symbols.

The ratio of the entropy of a source to the maximum value it could have while still restricted to the same
symbols will be called its relative entropy. This is the maximum compression possible when we encode into
the same alphabet. One minus the relative entropy is the redundancy. The redundancy of ordinary English,
not considering statistical structure over greater distances than about eight letters, is roughly 50%. This
means that when we write English half of what we write is determined by the structure of the language and
half is chosen freely. The figure 50% was found by several independent methods which all gave results in

14
