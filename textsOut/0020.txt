12. EQUIVOCATION AND CHANNEL CAPACITY

If the channel is noisy it is not in general possible to reconstruct the original message or the transmitted
signal with certainty by any operation on the received signal E. There are, however, ways of transmitting
the information which are optimal in combating noise. This is the problem which we now consider.

Suppose there are two possible symbols 0 and 1, and we are transmitting at a rate of 1000 symbols per
second with probabilities pop = py = 5. Thus our source is producing information at the rate of 1000 bits
per second. During transmission the noise introduces errors so that, on the average, | in 100 is received
incorrectly (a 0 as 1, or 1 as 0). What is the rate of transmission of information? Certainly less than 1000
bits per second since about 1% of the received symbols are incorrect. Our first impulse might be to say
the rate is 990 bits per second, merely subtracting the expected number of errors. This is not satisfactory
since it fails to take into account the recipient’s lack of knowledge of where the errors occur. We may carry
it to an extreme case and suppose the noise so great that the received symbols are entirely independent of
the transmitted symbols. The probability of receiving 1 is $ whatever was transmitted and similarly for 0.
Then about half of the received symbols are correct due to chance alone, and we would be giving the system
credit for transmitting 500 bits per second while actually no information is being transmitted at all. Equally
“good” transmission would be obtained by dispensing with the channel entirely and flipping a coin at the
receiving point.

Evidently the proper correction to apply to the amount of information transmitted is the amount of this
information which is missing in the received signal, or alternatively the uncertainty when we have received
a signal of what was actually sent. From our previous discussion of entropy as a measure of uncertainty it
seems reasonable to use the conditional entropy of the message, knowing the received signal, as a measure
of this missing information. This is indeed the proper definition, as we shall see later. Following this idea
the rate of actual transmission, R, would be obtained by subtracting from the rate of production (i.e., the
entropy of the source) the average rate of conditional entropy.

R=H(x)—Hy(x)

The conditional entropy Hy(x) will, for convenience, be called the equivocation. It measures the average
ambiguity of the received signal.

In the example considered above, if a 0 is received the a posteriori probability that a 0 was transmitted
is .99, and that a 1 was transmitted is .01. These figures are reversed if a | is received. Hence

Hy(x) = —[.99 log.99 + 0.01 log0.01]
= .081 bits/symbol

or 81 bits per second. We may say that the system is transmitting at a rate 1000 — 81 = 919 bits per second.
In the extreme case where a 0 is equally likely to be received as a 0 or 1 and similarly for 1, the a posteriori

probabilities are 3 5 and

Ay(x) = —[zlog 3 + 510g 5]
= | bit per symbol

or 1000 bits per second. The rate of transmission is then 0 as it should be.

The following theorem gives a direct intuitive interpretation of the equivocation and also serves to justify
it as the unique appropriate measure. We consider a communication system and an observer (or auxiliary
device) who can see both what is sent and what is recovered (with errors due to noise). This observer notes
the errors in the recovered message and transmits data to the receiving point over a “correction channel” to
enable the receiver to correct the errors. The situation is indicated schematically in Fig. 8.

Theorem 10: If the correction channel has a capacity equal to H,(x) it is possible to so encode the
correction data as to send it over this channel and correct all but an arbitrarily small fraction € of the errors.
This is not possible if the channel capacity is less than Hy(x).

20
